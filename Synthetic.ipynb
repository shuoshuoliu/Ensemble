{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saved file with    _ is from PSG\n",
    "#saved file without _ is from PSG2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from deslib.des import METADES\n",
    "from deslib.des import KNORAE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from REC import *\n",
    "random.seed(2021)\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simulation\n",
    "# a=np.random.rand(500,2)\n",
    "# b=np.random.rand(500,2)\n",
    "# w1=np.array([[-0.55,0],[0,0.23]])\n",
    "# w2=np.array([[1.55,0],[0,0.77]])\n",
    "# y=np.dot(a,w1)+np.dot(b,w2)\n",
    "# #y=NCImax(y)\n",
    "\n",
    "# data=np.concatenate((a,b)).reshape(2, *np.array(a).shape)\n",
    "# nrep=30\n",
    "# lmda=0.001\n",
    "# alpha=0.00001\n",
    "# M=2\n",
    "# K=2\n",
    "# finalW=full(nrep,M,K,data,y)\n",
    "# finalW=PSG(finalW,M,K,data,y,alpha,lmda)\n",
    "# print(finalW)\n",
    "\n",
    "# data=np.hstack([a,b])\n",
    "# reg=LinearRegression(fit_intercept=False).fit(data, y)\n",
    "# reg.coef_.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot\n",
    "# from pandas import DataFrame\n",
    "\n",
    "# X, y = make_moons(n_samples=1000,noise=0.2)\n",
    "# df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "# colors = {0:'red', 1:'blue'}\n",
    "# fig, ax = pyplot.subplots()\n",
    "# grouped = df.groupby('label')\n",
    "# for key, group in grouped:\n",
    "#     group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "# pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:41:48] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:41:54] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:41:57] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:01] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:07] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:11] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:18] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:22] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:25] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:29] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:35] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:39] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:42] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:46] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:50] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:53] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:42:56] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:43:00] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:43:03] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:43:07] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:43:10] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:43:14] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:43:17] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:43:21] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:43:24] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:43:28] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[20:43:31] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "rep=30\n",
    "ans=np.zeros((rep,8))\n",
    "\n",
    "for i in range(rep):\n",
    "    K=2 #number of classes\n",
    "    x, y = make_moons(n_samples=1000,noise=0.4)\n",
    "    dim=x.shape[1]\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30)\n",
    "    y_test=y_test+1\n",
    "    y_train_keep=y_train\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y_train)\n",
    "    encoded_Y = encoder.transform(y_train)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    y_train = np_utils.to_categorical(encoded_Y)\n",
    "    \n",
    "    model = XGBClassifier()\n",
    "    model.fit(x_train, y_train_keep) \n",
    "    P=model.predict_proba(x_train)\n",
    "    P=NCI2(P)\n",
    "    \n",
    "    model1 = GaussianNB()\n",
    "    model1.fit(x_train, y_train_keep)\n",
    "    P1=model1.predict_proba(x_train)\n",
    "    P1=NCI2(P1)\n",
    "    \n",
    "    model2=LogisticRegression(random_state=0).fit(x_train, y_train_keep)\n",
    "    P2=model2.predict_proba(x_train)\n",
    "    P2=NCI2(P2)\n",
    "    \n",
    "#     model3 = Sequential()\n",
    "#     model3.add(Dense(40, input_dim=dim, activation='relu'))\n",
    "#     model3.add(Dense(20, activation='relu'))\n",
    "#     model3.add(Dense(K, activation='softmax'))\n",
    "#     model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     model3.fit(x_train, y_train, epochs=150, batch_size=10,verbose=0)\n",
    "    \n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "    model3 = Sequential()\n",
    "    model3.add(Dense(40, input_dim=dim, activation='relu'))\n",
    "    model3.add(Dense(20, activation='relu'))\n",
    "    model3.add(Dense(K, activation='softmax'))\n",
    "    model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model3.fit(x_train, y_train, epochs=15, validation_split=0.3, callbacks=[callback], verbose=False)\n",
    "\n",
    "    P3=model3.predict(x_train)\n",
    "    P3=NCI2(P3)\n",
    "    \n",
    "    M=4 #number of candidates\n",
    "    #P0=NCIy(y_train,x_train,y_train_keep)\n",
    "    #P0=NCI2(P0)\n",
    "    P0=NCI2(y_train)\n",
    "    \n",
    "    T=model.predict_proba(x_test)\n",
    "    T1=model1.predict_proba(x_test)\n",
    "    T2=model2.predict_proba(x_test)\n",
    "    T3=model3.predict(x_test)\n",
    "    T=NCI2(T)\n",
    "    T1=NCI2(T1)\n",
    "    T2=NCI2(T2)\n",
    "    T3=NCI2(T3)\n",
    "    \n",
    "    data=np.concatenate((P,P1,P2,P3)).reshape(M, *np.array(P1).shape)\n",
    "    nrep=100\n",
    "    lmda=0.001\n",
    "    alpha=0.001 #0.01 to jump larger step\n",
    "    initW=full(nrep,M,K,data,P0)\n",
    "    finalW1=PSG(initW,M,K,data,P0,alpha,lmda)\n",
    "    finalW2=PSG2(initW,M,K,data,P0,alpha)\n",
    "    #lp1=pieceM(data,finalW1)\n",
    "    #lp2=pieceM(data,finalW2)\n",
    "    #use different nk since we don't know truth for test data\n",
    "#     T0=np.array(y_test)\n",
    "#     nk0=find_nK2(T0,K)\n",
    "#     T0=NCI(K,T0,nk0)\n",
    "    dt=np.concatenate((T,T1,T2,T3)).reshape(M, *np.array(T1).shape)\n",
    "    lp3=pieceM(dt,finalW1)\n",
    "    lp4=pieceM(dt,finalW2)\n",
    "    \n",
    "    ###################\n",
    "    X_train, X_dsel, y_train, y_dsel = train_test_split(x_train, y_train_keep,\n",
    "                                                    test_size=0.5,\n",
    "                                                    random_state=rng)\n",
    "    kne = KNORAE(random_state=rng)\n",
    "    meta = METADES(random_state=rng)\n",
    "    # Fitting the des techniques\n",
    "    kne.fit(X_dsel, y_dsel)\n",
    "    meta.fit(X_dsel, y_dsel)\n",
    "    \n",
    "    ans[i,:]=[accuracy_score(y_test, labelP1(T)),accuracy_score(y_test, labelP1(T1)),\n",
    "              accuracy_score(y_test, labelP1(T2)),accuracy_score(y_test, labelP1(T3)),\n",
    "              kne.score(x_test, y_test-1), meta.score(x_test, y_test-1),\n",
    "              accuracy_score(y_test, labelP1(lp3)),accuracy_score(y_test, labelP1(lp4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82966667 0.82733333 0.82866667 0.83522222 0.81788889 0.81888889\n",
      " 0.84844444 0.84833333]\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(ans,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0237479 , 0.02308439, 0.02323152, 0.02276585, 0.02173039,\n",
       "       0.01923217, 0.02156357, 0.02159818])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(ans, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName=\"synthetic\"+\".csv\"\n",
    "np.savetxt(fileName, ans, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
